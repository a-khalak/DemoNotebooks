{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc8640b-5d6e-40ad-8d9f-cc47d650535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66856071-8178-422f-ae6c-32bd64e5749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "234ec005-8940-4079-b1b6-e4faf61de217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e49e2cab-1ead-4571-a9ec-83ca9645e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b46cb-cb32-4d7f-b9ce-c5be0e6873b0",
   "metadata": {},
   "source": [
    "# Huggingface API test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e3dfe7d-cd88-420d-b7f9-0d8c4a817d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://huggingface.co/docs/api-inference/quicktour\n",
    "\n",
    "import json\n",
    "import requests\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\"}\n",
    "def query(payload):\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))\n",
    "data = query(\"Can you please let us know more details about your \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb89827e-0e8b-42f5-bd47-0319f2105466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Can you please let us know more details about your iphone?\\n\\nWe are currently working'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9021008e-0a72-4a9b-b238-9008bd1dc8e5",
   "metadata": {},
   "source": [
    "# Langchain example using API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a05b0852-aabd-4fff-a335-66f45f2972cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repo_id = \"google/flan-t5-xl\"\n",
    "repo_id = \"google/flan-t5-base\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b9b6791-249d-47ba-bb14-a41d699fb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0, \"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adebae7d-0f3e-4802-8c6e-393100368783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-xl', 'task': None, 'model_kwargs': {'temperature': 0, 'max_length': 64}}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73ed0601-98c4-4cbb-abc7-a7f0a3d578cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain, HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92906d70-b67d-4019-a768-bfc1b94a5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer; Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain= LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b941caec-9861-453f-bbc7-cbf5b67da562",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7b55880-e8dc-4c94-a2fb-394f9451c82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] output_parser=None partial_variables={} template=\"Question: {question}\\n\\nAnswer; Let's think step by step.\" template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3dfdedca-4082-4cb5-8b84-f1f09dd8fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28d246ba-7ca5-493a-9b4d-cb8e8a6100a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIFA World Cup was a football tournament played in the United States. FIFA World Cup was held in 1994. The answer: finland.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c923c27-dd1b-46fd-82b3-378a4372ce96",
   "metadata": {},
   "source": [
    "# Example using downloaded models\n",
    "\n",
    "Note this does not yet work on a MacMini b/c HuggingFacePipeline requires a CUDA enabled version of pytorch, which is not default for Mac.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ac43e14-4b45-4b22-86b9-995c1a4ccfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e17b5835-8c5f-4062-a105-f984a58cc7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'google/flan-t5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fc2d00d-5352-4fb6-84a3-ed865b0d6c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813f3276187d43909f8421f8627e6553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717c9556982444be8765a922c8e6f352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8119c098eab4e68844427aacdc49a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c623e00a7b649a084adbec0409b1fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb0d7f59-ca75-4dee-8f6e-a696c59fbe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2ce33c408e4186b1a7b7531e702025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6975eda3614d44e5b2e7b06f2d3cb193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Users/asif/.pyenv/versions/analysis/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary /Users/asif/.pyenv/versions/analysis/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Users/asif/.pyenv/versions/analysis/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/asif/.pyenv/versions/analysis/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/asif/.pyenv/versions/analysis/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/asif/.pyenv/versions/analysis/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asif/.pyenv/versions/analysis/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86438b0debe04568b5075ca9903db9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4d00bd3-d7b7-45a0-a12e-bb89dc46471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb90ef80-7a17-4832-9689-90b09150e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3251080-6af3-4db4-a973-88be70cfe679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teet w/o langchain\n",
    "#print(local_llm(\"What is the capital of California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632212f-93f0-4250-b7d9-cfb197666ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
